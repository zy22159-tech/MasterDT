{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "uYyjvnEWaNQo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYyjvnEWaNQo",
    "outputId": "e37929dc-22ca-401b-e3d0-8061a0406044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 135 kB 5.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 45.7 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608a53b9",
   "metadata": {
    "id": "608a53b9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from dateutil.parser import parse\n",
    "dateparse=lambda dates:parse(dates)\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "import keras_tuner as kt \n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "import IPython\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73M6I9aqQuK4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73M6I9aqQuK4",
    "outputId": "79886c16-8a08-4226-ceba-eea44dea84de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "'''from google.colab import drive\n",
    "drive.mount('/content/drive')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mnj9NoD39NZM",
   "metadata": {
    "id": "Mnj9NoD39NZM"
   },
   "source": [
    "### Pre-processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff898f7",
   "metadata": {
    "id": "cff898f7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv ('Data/weekly_features.csv')\n",
    "df = df.drop (columns = ['Unnamed: 0','USD_PHP Historical Data.csv'])\n",
    "dates = df.year*100+df.week\n",
    "df['Date'] = pd.to_datetime(dates.astype(str) + '0', format='%Y%W%w')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "mask = (df['Date'] >'1990-09-30') & (df['Date'] <= '2021-09-30')\n",
    "df= df.loc[mask]\n",
    "df = df.fillna(method='ffill')\n",
    "\n",
    "def convert_to_timestamp(x):\n",
    "    \"\"\"Convert date objects to integers\"\"\"\n",
    "    return time.mktime(x.to_datetime().timetuple())\n",
    "\n",
    "# https://www.aiproblog.com/index.php/2018/08/21/4-common-machine-learning-data-transforms-for-time-series-forecasting/\n",
    "# difference dataset\n",
    "diff_df=df.drop(columns=['Date', 'year', 'week'])\n",
    "diff_df = diff_df.diff()\n",
    "diff_df = diff_df.iloc[1:]\n",
    "#diff_df['year']=df.year[1:]\n",
    "diff_df['week']=df.week[1:]\n",
    "#diff_df['Date']=df.Date[1:]\n",
    "#diff_df['Date'] = pd.to_datetime(diff_df['Date'])\n",
    "# convert date to timestamp\n",
    "#diff_df['Date'] = diff_df['Date'].map(pd.Timestamp.timestamp)\n",
    "\n",
    "#split the data into training and testing dataset\n",
    "column_indices = {name: i for i, name in enumerate(diff_df.columns)}\n",
    "\n",
    "n = len(diff_df)\n",
    "train_df = diff_df[0:int(n*0.7)]\n",
    "test_df = diff_df[int(n*0.7):]\n",
    "\n",
    "num_features = diff_df.shape[1]\n",
    "\n",
    "#Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#col_list = [i for i in diff_df.columns if i != 'Date']\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train = scaler.fit_transform(train_df)\n",
    "scaled_test = scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf94eb3",
   "metadata": {
    "id": "1cf94eb3"
   },
   "outputs": [],
   "source": [
    "raw_values = df.drop(columns=['year','week'])\n",
    "raw_values['Date'] = pd.to_datetime(raw_values['Date'])\n",
    "# convert date to timestamp\n",
    "raw_values['Date'] = raw_values['Date'].map(pd.Timestamp.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc5c338",
   "metadata": {
    "id": "6cc5c338"
   },
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K1BCNkpZQDDE",
   "metadata": {
    "id": "K1BCNkpZQDDE"
   },
   "source": [
    "### Set up window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "UmhUKMxHQBw4",
   "metadata": {
    "id": "UmhUKMxHQBw4"
   },
   "outputs": [],
   "source": [
    "past_history = 20\n",
    "future_target = 15\n",
    "STEP = 1\n",
    "X, y = multivariate_data(scaled_train, scaled_train, 0,\n",
    "              None, past_history,\n",
    "              future_target, STEP,\n",
    "              single_step=False)\n",
    "\n",
    "X_test, y_test = multivariate_data(scaled_test, scaled_test, 0,\n",
    "                   None, past_history,\n",
    "                   future_target, STEP,\n",
    "                   single_step=False)\n",
    "test_data_multi= tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_data_multi = test_data_multi.batch(32).repeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JoCIZ-DOQMj5",
   "metadata": {
    "id": "JoCIZ-DOQMj5"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ADXO16eWxzff",
   "metadata": {
    "id": "ADXO16eWxzff"
   },
   "outputs": [],
   "source": [
    "class SampleModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        #model.add(tf.keras.layers.LSTM(units=hp.Int('units', min_value=20, max_value=600, step=20),return_sequences=True))\n",
    "        for i in range(hp.Int(\"n_cuDNNlstm_layers\",1,2)):\n",
    "          model.add(tf.compat.v1.keras.layers.CuDNNLSTM(units=hp.Int(f'lstm_{i}_units', min_value=192-128*i, max_value=384-256*i, step=32),return_sequences=True))\n",
    "          model.add(layers.Dropout(rate=hp.Float(f'lstm_{i}_dropout_rate', min_value=0.05, max_value=0.95, step=0.1)))\n",
    "\n",
    "        \n",
    "        for i in range(hp.Int(\"n_conv_layers\",1,2)):\n",
    "          model.add(layers.Conv1D(filters=hp.Int(f'conv_{i}_units', min_value=128-64*i, max_value=256-128*i, step=32), kernel_size=3, activation=\"relu\"))\n",
    "          model.add(layers.MaxPooling1D(pool_size=1))\n",
    "          model.add(layers.Dropout(rate=hp.Float(f'conv_{i}_dropout_rate', min_value=0.05, max_value=0.95, step=0.1)))\n",
    "        \n",
    "        model.add(layers.Flatten())\n",
    "       \n",
    "        for i in range(hp.Int(\"n_dense_layers\",1,2)):\n",
    "          model.add(layers.Dense(units=hp.Int(f'dense_{i}_units', min_value=384-256*i, max_value=512-256*i, step=32)))\n",
    "        model.add(layers.Dense(future_target*num_features,kernel_initializer=tf.initializers.zeros()))\n",
    "        model.add(layers.Reshape([future_target, num_features]))\n",
    "\n",
    "        model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-3, 1e-4, 1e-5])),\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.losses.MeanSquaredError()])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qLSpBxf19ouE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLSpBxf19ouE",
    "outputId": "956b45d4-ac95-49b1-dc49-398489cde355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 41s]\n",
      "val_loss: 0.025289083644747735\n",
      "\n",
      "Best val_loss So Far: 0.007745090406388044\n",
      "Total elapsed time: 00h 52m 32s\n",
      "{'n_cuDNNlstm_layers': 1, 'lstm_0_units': 320, 'lstm_0_dropout_rate': 0.05, 'n_conv_layers': 2, 'conv_0_units': 192, 'conv_0_dropout_rate': 0.25000000000000006, 'n_dense_layers': 2, 'dense_0_units': 416, 'learning_rate': 0.001, 'epochs': 80, 'lstm_1_units': 96, 'lstm_1_dropout_rate': 0.45000000000000007, 'conv_1_units': 96, 'conv_1_dropout_rate': 0.45000000000000007, 'dense_1_units': 256}\n"
     ]
    }
   ],
   "source": [
    "## trail.hyperparamter is replaced by hp\n",
    "buffer_size = 150\n",
    "\n",
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, x, y, *args, **kwargs):\n",
    "        splits = TimeSeriesSplit(n_splits=5)\n",
    "        val_losses = []\n",
    "        batch_size = 32\n",
    "        epochs = trial.hyperparameters.Int('epochs', 10, 100, step=10)\n",
    "\n",
    "        for train_indices, test_indices in splits.split(x):\n",
    "            x_train, x_val = x[train_indices], x[test_indices]\n",
    "            y_train, y_val = y[train_indices], y[test_indices]\n",
    "\n",
    "            train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            train_data = train_data.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "\n",
    "            val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            val_data = val_data.batch(batch_size).repeat()\n",
    "    \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "            val_loss= model.evaluate(x_val, y_val)\n",
    "            val_losses.append(val_loss[1])\n",
    "        \n",
    "        \n",
    "        self.oracle.update_trial(trial.trial_id, {'val_loss': np.mean(val_losses)})\n",
    "        #self.save_model(trial.trial_id, model)\n",
    "\n",
    "\n",
    "model = SampleModel()\n",
    "tuner = CVTuner(oracle=kt.oracles.RandomSearch(objective='val_loss',max_trials=60), hypermodel=model, directory='drive/MyDrive/Data', project_name = 'LSTM_CNN_encoder', executions_per_trial=2, overwrite = True)\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min') \n",
    "\n",
    "tuner.search(X, y, callbacks=[early_stopping])\n",
    "\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(best_hyperparameters.values)\n",
    "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
    "#best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ji-62pNApPPC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ji-62pNApPPC",
    "outputId": "774fd8bd-fc7d-437e-e602-7ad59d2c8ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in drive/MyDrive/Data/LSTM_CNN_encoder\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x7f7e9512cc50>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_cuDNNlstm_layers: 1\n",
      "lstm_0_units: 320\n",
      "lstm_0_dropout_rate: 0.05\n",
      "n_conv_layers: 2\n",
      "conv_0_units: 192\n",
      "conv_0_dropout_rate: 0.25000000000000006\n",
      "n_dense_layers: 2\n",
      "dense_0_units: 416\n",
      "learning_rate: 0.001\n",
      "epochs: 80\n",
      "lstm_1_units: 96\n",
      "lstm_1_dropout_rate: 0.45000000000000007\n",
      "conv_1_units: 96\n",
      "conv_1_dropout_rate: 0.45000000000000007\n",
      "dense_1_units: 256\n",
      "Score: 0.007745090406388044\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_cuDNNlstm_layers: 1\n",
      "lstm_0_units: 224\n",
      "lstm_0_dropout_rate: 0.25000000000000006\n",
      "n_conv_layers: 1\n",
      "conv_0_units: 160\n",
      "conv_0_dropout_rate: 0.25000000000000006\n",
      "n_dense_layers: 1\n",
      "dense_0_units: 480\n",
      "learning_rate: 0.001\n",
      "epochs: 80\n",
      "lstm_1_units: 96\n",
      "lstm_1_dropout_rate: 0.8500000000000002\n",
      "conv_1_units: 128\n",
      "conv_1_dropout_rate: 0.7500000000000002\n",
      "Score: 0.007887329161167144\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_cuDNNlstm_layers: 2\n",
      "lstm_0_units: 224\n",
      "lstm_0_dropout_rate: 0.5500000000000002\n",
      "n_conv_layers: 1\n",
      "conv_0_units: 224\n",
      "conv_0_dropout_rate: 0.6500000000000001\n",
      "n_dense_layers: 1\n",
      "dense_0_units: 448\n",
      "learning_rate: 0.001\n",
      "epochs: 90\n",
      "lstm_1_units: 96\n",
      "lstm_1_dropout_rate: 0.15000000000000002\n",
      "conv_1_units: 96\n",
      "conv_1_dropout_rate: 0.6500000000000001\n",
      "dense_1_units: 192\n",
      "Score: 0.00802707988768816\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_cuDNNlstm_layers: 1\n",
      "lstm_0_units: 224\n",
      "lstm_0_dropout_rate: 0.25000000000000006\n",
      "n_conv_layers: 1\n",
      "conv_0_units: 128\n",
      "conv_0_dropout_rate: 0.35000000000000003\n",
      "n_dense_layers: 1\n",
      "dense_0_units: 384\n",
      "learning_rate: 0.001\n",
      "epochs: 60\n",
      "lstm_1_units: 64\n",
      "lstm_1_dropout_rate: 0.25000000000000006\n",
      "conv_1_units: 128\n",
      "conv_1_dropout_rate: 0.8500000000000002\n",
      "dense_1_units: 224\n",
      "Score: 0.00803322046995163\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_cuDNNlstm_layers: 1\n",
      "lstm_0_units: 256\n",
      "lstm_0_dropout_rate: 0.05\n",
      "n_conv_layers: 2\n",
      "conv_0_units: 128\n",
      "conv_0_dropout_rate: 0.15000000000000002\n",
      "n_dense_layers: 2\n",
      "dense_0_units: 512\n",
      "learning_rate: 0.001\n",
      "epochs: 80\n",
      "lstm_1_units: 96\n",
      "lstm_1_dropout_rate: 0.05\n",
      "conv_1_units: 96\n",
      "conv_1_dropout_rate: 0.25000000000000006\n",
      "dense_1_units: 128\n",
      "Score: 0.008265944942831994\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_cuDNNlstm_layers: 1\n",
      "lstm_0_units: 384\n",
      "lstm_0_dropout_rate: 0.05\n",
      "n_conv_layers: 1\n",
      "conv_0_units: 128\n",
      "conv_0_dropout_rate: 0.6500000000000001\n",
      "n_dense_layers: 1\n",
      "dense_0_units: 384\n",
      "learning_rate: 0.001\n",
      "epochs: 50\n",
      "lstm_1_units: 128\n",
      "lstm_1_dropout_rate: 0.6500000000000001\n",
      "conv_1_units: 64\n",
      "conv_1_dropout_rate: 0.7500000000000002\n",
      "dense_1_units: 224\n",
      "Score: 0.008339188154786825\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_cuDNNlstm_layers: 1\n",
      "lstm_0_units: 320\n",
      "lstm_0_dropout_rate: 0.35000000000000003\n",
      "n_conv_layers: 2\n",
      "conv_0_units: 160\n",
      "conv_0_dropout_rate: 0.7500000000000002\n",
      "n_dense_layers: 2\n",
      "dense_0_units: 416\n",
      "learning_rate: 0.001\n",
      "epochs: 70\n",
      "lstm_1_units: 128\n",
      "lstm_1_dropout_rate: 0.45000000000000007\n",
      "conv_1_units: 64\n",
      "conv_1_dropout_rate: 0.8500000000000002\n",
      "dense_1_units: 224\n",
      "Score: 0.00872398829087615\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_cuDNNlstm_layers: 1\n",
      "lstm_0_units: 352\n",
      "lstm_0_dropout_rate: 0.35000000000000003\n",
      "n_conv_layers: 1\n",
      "conv_0_units: 224\n",
      "conv_0_dropout_rate: 0.15000000000000002\n",
      "n_dense_layers: 1\n",
      "dense_0_units: 512\n",
      "learning_rate: 0.001\n",
      "epochs: 50\n",
      "lstm_1_units: 96\n",
      "lstm_1_dropout_rate: 0.25000000000000006\n",
      "conv_1_units: 96\n",
      "conv_1_dropout_rate: 0.15000000000000002\n",
      "dense_1_units: 224\n",
      "Score: 0.008901266288012267\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_cuDNNlstm_layers: 1\n",
      "lstm_0_units: 352\n",
      "lstm_0_dropout_rate: 0.5500000000000002\n",
      "n_conv_layers: 1\n",
      "conv_0_units: 160\n",
      "conv_0_dropout_rate: 0.35000000000000003\n",
      "n_dense_layers: 1\n",
      "dense_0_units: 480\n",
      "learning_rate: 0.001\n",
      "epochs: 60\n",
      "lstm_1_units: 96\n",
      "lstm_1_dropout_rate: 0.9500000000000002\n",
      "conv_1_units: 96\n",
      "conv_1_dropout_rate: 0.35000000000000003\n",
      "dense_1_units: 192\n",
      "Score: 0.009505326673388482\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_cuDNNlstm_layers: 1\n",
      "lstm_0_units: 256\n",
      "lstm_0_dropout_rate: 0.25000000000000006\n",
      "n_conv_layers: 2\n",
      "conv_0_units: 224\n",
      "conv_0_dropout_rate: 0.15000000000000002\n",
      "n_dense_layers: 2\n",
      "dense_0_units: 448\n",
      "learning_rate: 0.001\n",
      "epochs: 30\n",
      "lstm_1_units: 128\n",
      "lstm_1_dropout_rate: 0.05\n",
      "conv_1_units: 128\n",
      "conv_1_dropout_rate: 0.35000000000000003\n",
      "dense_1_units: 160\n",
      "Score: 0.009726025629788637\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kMxD3VKY6DMD",
   "metadata": {
    "id": "kMxD3VKY6DMD"
   },
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tmYABWiq5eKv",
   "metadata": {
    "id": "tmYABWiq5eKv"
   },
   "outputs": [],
   "source": [
    "class SampleModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.compat.v1.keras.layers.CuDNNLSTM(units=hp.Int(f'lstm_units', min_value=128, max_value=256, step=64),return_sequences=False))\n",
    "        model.add(layers.Dense(future_target*num_features,kernel_initializer=tf.initializers.zeros()))\n",
    "        model.add(layers.Reshape([future_target, num_features]))\n",
    "\n",
    "        model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.losses.MeanSquaredError()])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8Jgs35uy51lM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Jgs35uy51lM",
    "outputId": "9d531654-9366-42b3-8b71-2b4ad08ed9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 57s]\n",
      "val_loss: 0.007861247286200523\n",
      "\n",
      "Best val_loss So Far: 0.007741204462945461\n",
      "Total elapsed time: 00h 03m 21s\n",
      "{'lstm_units': 256}\n"
     ]
    }
   ],
   "source": [
    "buffer_size = 150\n",
    "\n",
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, x, y, *args, **kwargs):\n",
    "        splits = TimeSeriesSplit(n_splits=5)\n",
    "        val_losses = []\n",
    "        batch_size = 32\n",
    "        epochs = 80\n",
    "\n",
    "        for train_indices, test_indices in splits.split(x):\n",
    "            x_train, x_val = x[train_indices], x[test_indices]\n",
    "            y_train, y_val = y[train_indices], y[test_indices]\n",
    "\n",
    "            train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            train_data = train_data.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "\n",
    "            val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            val_data = val_data.batch(batch_size).repeat()\n",
    "    \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "            val_loss= model.evaluate(x_val, y_val)\n",
    "            val_losses.append(val_loss[1])\n",
    "        \n",
    "        \n",
    "        self.oracle.update_trial(trial.trial_id, {'val_loss': np.mean(val_losses)})\n",
    "        #self.save_model(trial.trial_id, model)\n",
    "\n",
    "\n",
    "model = SampleModel()\n",
    "tuner = CVTuner(oracle=kt.oracles.RandomSearch(objective='val_loss',max_trials=10), hypermodel=model, directory='drive/MyDrive/Data', project_name = 'LSTM', executions_per_trial=2, overwrite = True)\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min') \n",
    "\n",
    "tuner.search(X, y, callbacks=[early_stopping])\n",
    "\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(best_hyperparameters.values)\n",
    "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
    "#best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qjvpBqKqD7G-",
   "metadata": {
    "id": "qjvpBqKqD7G-"
   },
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2l-XUCdnCgdR",
   "metadata": {
    "id": "2l-XUCdnCgdR"
   },
   "outputs": [],
   "source": [
    "CONV_WIDTH = 3\n",
    "class SampleModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]))\n",
    "        model.add(layers.Conv1D(filters=hp.Int(f'conv_units', min_value=32, max_value=512, step=32), kernel_size=3, activation=\"relu\"))\n",
    "        model.add(layers.Dropout(rate=hp.Float(f'conv_dropout_rate', min_value=0.05, max_value=0.95, step=0.1)))\n",
    "        # Shape => [batch, 1,  out_steps*features]\n",
    "        model.add(layers.Dense(future_target*num_features,kernel_initializer=tf.initializers.zeros()))\n",
    "        model.add(layers.Reshape([future_target, num_features]))\n",
    "      \n",
    "        model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.losses.MeanSquaredError()])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RN_iwkxyD24h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RN_iwkxyD24h",
    "outputId": "51004d64-7c60-498e-de73-8c6959dacf72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 40s]\n",
      "val_loss: 0.010974389687180518\n",
      "\n",
      "Best val_loss So Far: 0.009597768541425467\n",
      "Total elapsed time: 00h 19m 43s\n",
      "{'conv_units': 480, 'conv_dropout_rate': 0.25000000000000006}\n"
     ]
    }
   ],
   "source": [
    "buffer_size = 150\n",
    "\n",
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, x, y, *args, **kwargs):\n",
    "        splits = TimeSeriesSplit(n_splits=5)\n",
    "        val_losses = []\n",
    "        batch_size = 32\n",
    "        epochs = 80\n",
    "\n",
    "        for train_indices, test_indices in splits.split(x):\n",
    "            x_train, x_val = x[train_indices], x[test_indices]\n",
    "            y_train, y_val = y[train_indices], y[test_indices]\n",
    "\n",
    "            train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            train_data = train_data.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "\n",
    "            val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            val_data = val_data.batch(batch_size).repeat()\n",
    "    \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "            val_loss= model.evaluate(x_val, y_val)\n",
    "            val_losses.append(val_loss[1])\n",
    "        \n",
    "        \n",
    "        self.oracle.update_trial(trial.trial_id, {'val_loss': np.mean(val_losses)})\n",
    "        #self.save_model(trial.trial_id, model)\n",
    "\n",
    "\n",
    "model = SampleModel()\n",
    "tuner = CVTuner(oracle=kt.oracles.RandomSearch(objective='val_loss',max_trials=30), hypermodel=model, directory='drive/MyDrive/Data', project_name = 'CNN', executions_per_trial=2, overwrite = True)\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min') \n",
    "\n",
    "tuner.search(X, y, callbacks=[early_stopping])\n",
    "\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(best_hyperparameters.values)\n",
    "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
    "#best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
