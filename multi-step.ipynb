{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4450d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from dateutil.parser import parse\n",
    "dateparse=lambda dates:parse(dates)\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df29ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('Data/weekly_features.csv')\n",
    "df = df.drop (columns = ['Unnamed: 0','USD_PHP Historical Data.csv'])\n",
    "dates = df.year*100+df.week\n",
    "df['Date'] = pd.to_datetime(dates.astype(str) + '0', format='%Y%W%w')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "mask = (df['Date'] >'1990-09-30') & (df['Date'] <= '2021-09-30')\n",
    "df= df.loc[mask]\n",
    "df = df.fillna(method='ffill')\n",
    "\n",
    "def convert_to_timestamp(x):\n",
    "    \"\"\"Convert date objects to integers\"\"\"\n",
    "    return time.mktime(x.to_datetime().timetuple())\n",
    "\n",
    "# https://www.aiproblog.com/index.php/2018/08/21/4-common-machine-learning-data-transforms-for-time-series-forecasting/\n",
    "# difference dataset\n",
    "diff_df=df.drop(columns=['Date', 'year', 'week'])\n",
    "diff_df = diff_df.diff()\n",
    "diff_df = diff_df.iloc[1:]\n",
    "#diff_df['year']=df.year[1:]\n",
    "diff_df['week']=df.week[1:]\n",
    "#diff_df['Date']=df.Date[1:]\n",
    "#diff_df['Date'] = pd.to_datetime(diff_df['Date'])\n",
    "# convert date to timestamp\n",
    "#diff_df['Date'] = diff_df['Date'].map(pd.Timestamp.timestamp)\n",
    "\n",
    "#split the data into training and testing dataset\n",
    "column_indices = {name: i for i, name in enumerate(diff_df.columns)}\n",
    "\n",
    "n = len(diff_df)\n",
    "train_df = diff_df[0:int(n*0.7)]\n",
    "test_df = diff_df[int(n*0.7):]\n",
    "\n",
    "num_features = diff_df.shape[1]\n",
    "\n",
    "#Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#col_list = [i for i in diff_df.columns if i != 'Date']\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train = scaler.fit_transform(train_df)\n",
    "scaled_test = scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d446dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(data = scaled_train, columns = ['tmax','tmin','prcp','Crude Oil WTI', 'Gold','Oats','Corn','Soybeans','Wheat','USD_CAD','USD_CNY','USD_EUR','USD_MXN','SWE','SNOW','SNWD','year','week'])\n",
    "test_df = pd.DataFrame(data = scaled_test, columns = ['tmax','tmin','prcp','Crude Oil WTI', 'Gold','Oats','Corn','Soybeans','Wheat','USD_CAD','USD_CNY','USD_EUR','USD_MXN','SWE','SNOW','SNWD','year', 'week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb98a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>prcp</th>\n",
       "      <th>Crude Oil WTI</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Oats</th>\n",
       "      <th>Corn</th>\n",
       "      <th>Soybeans</th>\n",
       "      <th>Wheat</th>\n",
       "      <th>USD_CAD</th>\n",
       "      <th>USD_CNY</th>\n",
       "      <th>USD_EUR</th>\n",
       "      <th>USD_MXN</th>\n",
       "      <th>SWE</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.231719</td>\n",
       "      <td>-2.491029</td>\n",
       "      <td>-17.361429</td>\n",
       "      <td>-2.800000</td>\n",
       "      <td>-20.660000</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>-5.40</td>\n",
       "      <td>-14.700</td>\n",
       "      <td>-11.00</td>\n",
       "      <td>0.01622</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00740</td>\n",
       "      <td>0.00600</td>\n",
       "      <td>40.771429</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>1.537537</td>\n",
       "      <td>1990</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-3.653096</td>\n",
       "      <td>-3.812763</td>\n",
       "      <td>1.347143</td>\n",
       "      <td>-5.612000</td>\n",
       "      <td>2.880000</td>\n",
       "      <td>1.6500</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-4.900</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00418</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>74.728571</td>\n",
       "      <td>-488.571429</td>\n",
       "      <td>0.181753</td>\n",
       "      <td>1990</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.996899</td>\n",
       "      <td>1.217759</td>\n",
       "      <td>-13.064286</td>\n",
       "      <td>3.506000</td>\n",
       "      <td>5.940000</td>\n",
       "      <td>-2.5000</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-14.950</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.00392</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>139.442857</td>\n",
       "      <td>11478.000000</td>\n",
       "      <td>1.356229</td>\n",
       "      <td>1990</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-12.418266</td>\n",
       "      <td>-6.051888</td>\n",
       "      <td>23.538571</td>\n",
       "      <td>-0.860000</td>\n",
       "      <td>6.760000</td>\n",
       "      <td>-0.5500</td>\n",
       "      <td>1.30</td>\n",
       "      <td>-14.450</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.01114</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>254.800000</td>\n",
       "      <td>15332.571429</td>\n",
       "      <td>6.229240</td>\n",
       "      <td>1990</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>7.199344</td>\n",
       "      <td>1.608928</td>\n",
       "      <td>-27.015714</td>\n",
       "      <td>-2.454000</td>\n",
       "      <td>-4.120000</td>\n",
       "      <td>-9.0000</td>\n",
       "      <td>-4.65</td>\n",
       "      <td>-15.000</td>\n",
       "      <td>-9.90</td>\n",
       "      <td>-0.00130</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00472</td>\n",
       "      <td>0.00400</td>\n",
       "      <td>331.885714</td>\n",
       "      <td>-609.571429</td>\n",
       "      <td>-0.468038</td>\n",
       "      <td>1990</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>2.259412</td>\n",
       "      <td>0.171195</td>\n",
       "      <td>-5.414286</td>\n",
       "      <td>2.316000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>-5.80</td>\n",
       "      <td>-3.250</td>\n",
       "      <td>-16.25</td>\n",
       "      <td>-0.00746</td>\n",
       "      <td>-0.01200</td>\n",
       "      <td>-0.00356</td>\n",
       "      <td>0.16694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2021</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>-4.788346</td>\n",
       "      <td>-3.675091</td>\n",
       "      <td>11.850000</td>\n",
       "      <td>1.814000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>-8.3000</td>\n",
       "      <td>-26.10</td>\n",
       "      <td>-52.050</td>\n",
       "      <td>-11.55</td>\n",
       "      <td>-0.00446</td>\n",
       "      <td>-0.01714</td>\n",
       "      <td>-0.00548</td>\n",
       "      <td>-0.24268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2021</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>1.073948</td>\n",
       "      <td>-2.842894</td>\n",
       "      <td>-24.675714</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>-13.903333</td>\n",
       "      <td>-13.5625</td>\n",
       "      <td>-19.40</td>\n",
       "      <td>-15.624</td>\n",
       "      <td>-9.50</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>-0.00328</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>-0.09716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2021</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>-1.637910</td>\n",
       "      <td>0.823106</td>\n",
       "      <td>12.104286</td>\n",
       "      <td>0.731333</td>\n",
       "      <td>-23.576667</td>\n",
       "      <td>35.9125</td>\n",
       "      <td>14.60</td>\n",
       "      <td>11.624</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.00400</td>\n",
       "      <td>-0.00672</td>\n",
       "      <td>0.00346</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2021</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>-4.110992</td>\n",
       "      <td>-5.153240</td>\n",
       "      <td>3.267143</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>-16.460000</td>\n",
       "      <td>29.2500</td>\n",
       "      <td>6.05</td>\n",
       "      <td>-7.000</td>\n",
       "      <td>8.95</td>\n",
       "      <td>0.00586</td>\n",
       "      <td>0.01242</td>\n",
       "      <td>0.00476</td>\n",
       "      <td>0.17294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.714286</td>\n",
       "      <td>0.013692</td>\n",
       "      <td>2021</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1642 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tmax      tmin       prcp  Crude Oil WTI       Gold     Oats  \\\n",
       "36     0.231719 -2.491029 -17.361429      -2.800000 -20.660000   0.0500   \n",
       "37    -3.653096 -3.812763   1.347143      -5.612000   2.880000   1.6500   \n",
       "38     2.996899  1.217759 -13.064286       3.506000   5.940000  -2.5000   \n",
       "39   -12.418266 -6.051888  23.538571      -0.860000   6.760000  -0.5500   \n",
       "40     7.199344  1.608928 -27.015714      -2.454000  -4.120000  -9.0000   \n",
       "...         ...       ...        ...            ...        ...      ...   \n",
       "1673   2.259412  0.171195  -5.414286       2.316000  17.760000  14.5000   \n",
       "1674  -4.788346 -3.675091  11.850000       1.814000  14.000000  -8.3000   \n",
       "1675   1.073948 -2.842894 -24.675714       0.926667 -13.903333 -13.5625   \n",
       "1676  -1.637910  0.823106  12.104286       0.731333 -23.576667  35.9125   \n",
       "1677  -4.110992 -5.153240   3.267143       0.318000 -16.460000  29.2500   \n",
       "\n",
       "       Corn  Soybeans  Wheat  USD_CAD  USD_CNY  USD_EUR  USD_MXN         SWE  \\\n",
       "36    -5.40   -14.700 -11.00  0.01622  0.00000 -0.00740  0.00600   40.771429   \n",
       "37     2.70    -4.900   0.15  0.00418  0.00000  0.00004  0.00800   74.728571   \n",
       "38    -0.90   -14.950  -0.50 -0.00392  0.00000  0.00008  0.00200  139.442857   \n",
       "39     1.30   -14.450   3.10 -0.00002  0.00000 -0.01114  0.00200  254.800000   \n",
       "40    -4.65   -15.000  -9.90 -0.00130  0.00000 -0.00472  0.00400  331.885714   \n",
       "...     ...       ...    ...      ...      ...      ...      ...         ...   \n",
       "1673  -5.80    -3.250 -16.25 -0.00746 -0.01200 -0.00356  0.16694    0.000000   \n",
       "1674 -26.10   -52.050 -11.55 -0.00446 -0.01714 -0.00548 -0.24268    0.000000   \n",
       "1675 -19.40   -15.624  -9.50  0.00632 -0.00328  0.00050 -0.09716    0.000000   \n",
       "1676  14.60    11.624   1.20  0.00400 -0.00672  0.00346  0.00142    0.000000   \n",
       "1677   6.05    -7.000   8.95  0.00586  0.01242  0.00476  0.17294    0.000000   \n",
       "\n",
       "              SNOW      SNWD  year  week  \n",
       "36       97.000000  1.537537  1990    41  \n",
       "37     -488.571429  0.181753  1990    42  \n",
       "38    11478.000000  1.356229  1990    43  \n",
       "39    15332.571429  6.229240  1990    44  \n",
       "40     -609.571429 -0.468038  1990    45  \n",
       "...            ...       ...   ...   ...  \n",
       "1673     -7.285714  0.000000  2021    34  \n",
       "1674      0.000000  0.000000  2021    35  \n",
       "1675      0.000000  0.000000  2021    36  \n",
       "1676      0.000000  0.000000  2021    37  \n",
       "1677     11.714286  0.013692  2021    38  \n",
       "\n",
       "[1642 rows x 18 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc5b57",
   "metadata": {},
   "source": [
    "#### raw values generate from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b73d4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_values = df.drop(columns=['Date'])\n",
    "#raw_values['Date'] = pd.to_datetime(raw_values['Date'])\n",
    "# convert date to timestamp\n",
    "#raw_values['Date'] = raw_values['Date'].map(pd.Timestamp.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ce58e",
   "metadata": {},
   "source": [
    "### sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205b7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded32a2",
   "metadata": {},
   "source": [
    "### compile and fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc55456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of training models\n",
    "MAX_EPOCHS = 100\n",
    "EVALUATION_INTERVAL = 200\n",
    "batch_size = 32\n",
    "buffer_size = 150\n",
    "def compile_and_fit(model, train, val, patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min') \n",
    "\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.losses.MeanSquaredError()])\n",
    "\n",
    "    history = model.fit(train, batch_size=batch_size, epochs=MAX_EPOCHS,\n",
    "                      validation_data=val,\n",
    "                        steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      callbacks=[early_stopping],\n",
    "                    validation_steps=10\n",
    "                       )\n",
    "    model.reset_states()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5648d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X, past_history):\n",
    "    X = X.reshape(1, past_history, num_features)\n",
    "    yhat = model.predict(X)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74329874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_difference(history, yhat, position=1):\n",
    "    return yhat + history[position]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81985b",
   "metadata": {},
   "source": [
    "### set up window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5bfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = 20\n",
    "future_target = 5\n",
    "STEP = 1\n",
    "X_multi, y_multi = multivariate_data(scaled_train, scaled_train, 0,\n",
    "                                                   None, past_history,\n",
    "                                                   future_target, STEP,\n",
    "                                                   single_step=False)\n",
    "\n",
    "X_test_multi, y_test_multi = multivariate_data(scaled_test, scaled_test, 0,\n",
    "                                                   None, past_history,\n",
    "                                                   future_target, STEP,\n",
    "                                                   single_step=False)\n",
    "test_data_multi= tf.data.Dataset.from_tensor_slices((X_test_multi, y_test_multi))\n",
    "test_data_multi = test_data_multi.batch(batch_size).repeat()\n",
    "\n",
    "multi_val_performance = {}\n",
    "multi_test_performance = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1281b27",
   "metadata": {},
   "source": [
    "### split data into multiple training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b30faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Observations: 376\n",
      "Training Observations: 189\n",
      "Testing Observations: 187\n",
      "\n",
      "\n",
      "Fold: 1\n",
      "Observations: 563\n",
      "Training Observations: 376\n",
      "Testing Observations: 187\n",
      "\n",
      "\n",
      "Fold: 2\n",
      "Observations: 750\n",
      "Training Observations: 563\n",
      "Testing Observations: 187\n",
      "\n",
      "\n",
      "Fold: 3\n",
      "Observations: 937\n",
      "Training Observations: 750\n",
      "Testing Observations: 187\n",
      "\n",
      "\n",
      "Fold: 4\n",
      "Observations: 1124\n",
      "Training Observations: 937\n",
      "Testing Observations: 187\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/code/tomwarrens/timeseriessplit-how-to-use-it/notebook \n",
    "splits = TimeSeriesSplit(n_splits=5)\n",
    "for fold, (train_index, test_index) in enumerate(splits.split(X_multi)):\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "    X_train, X_val = X_multi[train_index], X_multi[test_index]\n",
    "    #X_train, X_val = scaled_train[train_index], scaled_train[test_index]\n",
    "    #print(\"TRAIN indices:\", train_index, \"\\n\", \"TEST indices:\", test_index)\n",
    "    print('Observations: %d' % (len(X_train) + len(X_val)))\n",
    "    print('Training Observations: %d' % (len(X_train)))\n",
    "    print('Testing Observations: %d' % (len(X_val)))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5492169b",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22132527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return tf.tile(inputs[:, -1:, :], [1, future_target, 1])\n",
    "\n",
    "baseline = MultiStepLastBaseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c832b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_val_list = {}\n",
    "for fold, (train_index, test_index) in enumerate(splits.split(X_multi)):\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "    x_train_multi, x_val_multi = X_multi[train_index], X_multi[test_index]\n",
    "    y_train_multi, y_val_multi = y_multi[train_index], y_multi[test_index]\n",
    "    print('Observations: %d' % (len(x_train_multi) + len(x_val_multi)))\n",
    "    print('Training Observations: %d' % (len(x_train_multi)))\n",
    "    print('Testing Observations: %d' % (len(x_val_multi)))\n",
    "    train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "    train_data_multi = train_data_multi.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "\n",
    "    val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "    val_data_multi = val_data_multi.batch(batch_size).repeat()\n",
    "    \n",
    "    baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    print(y_train_multi.shape)\n",
    "    evaluation_val_list [\"Fold {}\".format(fold)] = baseline.evaluate(val_data_multi, steps=50)\n",
    "\n",
    "\n",
    "\n",
    "IPython.display.clear_output()\n",
    "mae = []\n",
    "# calculate the mean mae\n",
    "for v in evaluation_val_list.values():\n",
    "    mae.append(v[1])\n",
    "Mean_mae = mean(mae)\n",
    "multi_val_performance['Baseline'] = Mean_mae\n",
    "multi_test_performance['Baseline'] = baseline.evaluate(test_data_multi, verbose=0, steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f88a5",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1a7678e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "arima_mse = []\n",
    "for i in range(num_features):\n",
    "    model1 = pm.ARIMA(order=(2, 1, 0))\n",
    "    #model2 = pm.ARIMA(order=(1, 1, 2),\n",
    "                  #seasonal_order=(0, 1, 1, 12),\n",
    "                  #suppress_warnings=True)\n",
    "    cv = model_selection.SlidingWindowForecastCV(window_size=20, step=1, h=5)\n",
    "    model1_cv_scores = model_selection.cross_val_score(\n",
    "    model1, scaled_test[:468,i], scoring='mean_squared_error', cv=cv, verbose=2)\n",
    "    m1_average_error = np.average(model1_cv_scores)\n",
    "    arima_mse.append(m1_average_error)\n",
    "IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800f6c7",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bfb12bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units].\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features].\n",
    "    tf.keras.layers.Dense(future_target*num_features, kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features].\n",
    "    tf.keras.layers.Reshape([future_target, num_features])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4af5c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_val_list = {}\n",
    "for fold, (train_index, test_index) in enumerate(splits.split(X_multi)):\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "    x_train_multi, x_val_multi = X_multi[train_index], X_multi[test_index]\n",
    "    y_train_multi, y_val_multi = y_multi[train_index], y_multi[test_index]\n",
    "    print('Observations: %d' % (len(x_train_multi) + len(x_val_multi)))\n",
    "    print('Training Observations: %d' % (len(x_train_multi)))\n",
    "    print('Testing Observations: %d' % (len(x_val_multi)))\n",
    "    train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "    train_data_multi = train_data_multi.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "\n",
    "    val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "    val_data_multi = val_data_multi.batch(batch_size).repeat()\n",
    "    \n",
    "    history = compile_and_fit(multi_lstm_model, train_data_multi, val_data_multi)\n",
    "\n",
    "    evaluation_val_list [\"Fold {}\".format(fold)] = multi_lstm_model.evaluate(val_data_multi, steps=50)\n",
    "\n",
    "\n",
    "\n",
    "IPython.display.clear_output()\n",
    "mae = []\n",
    "# calculate the mean mae\n",
    "for v in evaluation_val_list.values():\n",
    "    mae.append(v[1])\n",
    "Mean_mae = mean(mae)\n",
    "multi_val_performance['LSTM'] = Mean_mae\n",
    "multi_test_performance['LSTM'] = multi_lstm_model.evaluate(test_data_multi, verbose=0, steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da92cd6",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29b20ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_WIDTH = 3\n",
    "multi_conv_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "    # Shape => [batch, 1, dense_units]\n",
    "    #tf.keras.layers.Dense(64, activation='relu'),\n",
    "    # Shape => [batch, 1, conv_units]\n",
    "    tf.keras.layers.Conv1D(64, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "    # Shape => [batch, 1,  out_steps*features]\n",
    "    tf.keras.layers.Dense(future_target*num_features,kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([future_target, num_features])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdf9659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_val_list = {}\n",
    "for fold, (train_index, test_index) in enumerate(splits.split(X_multi)):\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "    x_train_multi, x_val_multi = X_multi[train_index], X_multi[test_index]\n",
    "    y_train_multi, y_val_multi = y_multi[train_index], y_multi[test_index]\n",
    "    print('Observations: %d' % (len(x_train_multi) + len(x_val_multi)))\n",
    "    print('Training Observations: %d' % (len(x_train_multi)))\n",
    "    print('Testing Observations: %d' % (len(x_val_multi)))\n",
    "    train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "    train_data_multi = train_data_multi.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "\n",
    "    val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "    val_data_multi = val_data_multi.batch(batch_size).repeat()\n",
    "    \n",
    "    history = compile_and_fit(multi_conv_model, train_data_multi, val_data_multi)\n",
    "\n",
    "    evaluation_val_list [\"Fold {}\".format(fold)] = multi_conv_model.evaluate(val_data_multi, steps=50)\n",
    "\n",
    "\n",
    "\n",
    "IPython.display.clear_output()\n",
    "mae = []\n",
    "# calculate the mean mae\n",
    "for v in evaluation_val_list.values():\n",
    "    mae.append(v[1])\n",
    "Mean_mae = mean(mae)\n",
    "multi_val_performance['CNN'] = Mean_mae\n",
    "multi_test_performance['CNN'] = multi_conv_model.evaluate(test_data_multi, verbose=0, steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66acf46",
   "metadata": {},
   "source": [
    "### LSTM-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "501f991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lstm_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(224,input_shape= (past_history,num_features), return_sequences=True),\n",
    "    tf.keras.layers.Dropout(rate = 0.55),\n",
    "    tf.keras.layers.LSTM(96,input_shape= (past_history,num_features), return_sequences=True),\n",
    "    tf.keras.layers.Dropout(rate = 0.15),\n",
    "    tf.keras.layers.Conv1D(filters=224, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=1),\n",
    "    tf.keras.layers.Dropout(rate = 0.65),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(448, activation='relu'),\n",
    "    tf.keras.layers.Dense(future_target*num_features, kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features].\n",
    "    tf.keras.layers.Reshape([future_target, num_features]) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f402a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 [==============================] - 10s 59ms/step - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0061 - mean_squared_error: 0.0062\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 9/100\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 10/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 11/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 12/100\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 13/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 14/100\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 15/100\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 16/100\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 0.0056 - mean_squared_error: 0.0055\n",
      "Epoch 17/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 18/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 19/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 20/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 21/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 22/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0055 - mean_squared_error: 0.0056\n",
      "Epoch 23/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 24/100\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 0.0054 - mean_squared_error: 0.0055\n",
      "Epoch 25/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 26/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 27/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 28/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 29/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 30/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 31/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 32/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 33/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 34/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 35/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 36/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0053 - mean_squared_error: 0.0052\n",
      "Epoch 37/100\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 38/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 39/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 40/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 41/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 42/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 43/100\n",
      "36/36 [==============================] - 2s 55ms/step - loss: 0.0051 - mean_squared_error: 0.0050\n",
      "Epoch 44/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 45/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 46/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 47/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 48/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 49/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 50/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 51/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0051 - mean_squared_error: 0.0052\n",
      "Epoch 52/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 53/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0050 - mean_squared_error: 0.0049\n",
      "Epoch 54/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 55/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 56/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 57/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 58/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 59/100\n",
      "36/36 [==============================] - 2s 55ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 60/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 61/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0049 - mean_squared_error: 0.0048\n",
      "Epoch 62/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0048 - mean_squared_error: 0.0049\n",
      "Epoch 63/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0049 - mean_squared_error: 0.0050\n",
      "Epoch 64/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 65/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 66/100\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 0.0049 - mean_squared_error: 0.0048\n",
      "Epoch 67/100\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 68/100\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 69/100\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 70/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 71/100\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 72/100\n",
      "36/36 [==============================] - 2s 52ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 73/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 74/100\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0049 - mean_squared_error: 0.0048\n",
      "Epoch 75/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 76/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0048 - mean_squared_error: 0.0047\n",
      "Epoch 77/100\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0048 - mean_squared_error: 0.0049\n",
      "Epoch 78/100\n",
      "36/36 [==============================] - 2s 46ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 79/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 80/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0047 - mean_squared_error: 0.0049\n",
      "Epoch 81/100\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 82/100\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 83/100\n",
      "36/36 [==============================] - 2s 46ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 84/100\n",
      "36/36 [==============================] - 2s 46ms/step - loss: 0.0048 - mean_squared_error: 0.0047\n",
      "Epoch 85/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 86/100\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0050 - mean_squared_error: 0.0049\n",
      "Epoch 87/100\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 88/100\n",
      "36/36 [==============================] - 2s 55ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 89/100\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 90/100\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 91/100\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 92/100\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0048 - mean_squared_error: 0.0047\n",
      "Epoch 93/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0047 - mean_squared_error: 0.0048\n",
      "Epoch 94/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 95/100\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 96/100\n",
      "36/36 [==============================] - 2s 46ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 97/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 98/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 99/100\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0046 - mean_squared_error: 0.0045\n",
      "Epoch 100/100\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n"
     ]
    }
   ],
   "source": [
    "evaluation_val_list = {}\n",
    "for fold, (train_index, test_index) in enumerate(splits.split(X_multi)):\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "    x_train_multi, x_val_multi = X_multi[train_index], X_multi[test_index]\n",
    "    y_train_multi, y_val_multi = y_multi[train_index], y_multi[test_index]\n",
    "    print('Observations: %d' % (len(x_train_multi) + len(x_val_multi)))\n",
    "    print('Training Observations: %d' % (len(x_train_multi)))\n",
    "    print('Testing Observations: %d' % (len(x_val_multi)))\n",
    "    train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "    train_data_multi = train_data_multi.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "\n",
    "    val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "    val_data_multi = val_data_multi.batch(batch_size).repeat()\n",
    "    \n",
    "    history = compile_and_fit(multi_lstm_cnn, train_data_multi, val_data_multi)\n",
    "\n",
    "    evaluation_val_list [\"Fold {}\".format(fold)] = multi_lstm_cnn.evaluate(val_data_multi, steps=50)\n",
    "\n",
    "\n",
    "\n",
    "IPython.display.clear_output()\n",
    "mae = []\n",
    "# calculate the mean mae\n",
    "for v in evaluation_val_list.values():\n",
    "    mae.append(v[1])\n",
    "Mean_mae = mean(mae)\n",
    "multi_val_performance['LSTM-CNN'] = Mean_mae\n",
    "\n",
    "multi_lstm_cnn.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.losses.MeanSquaredError()])\n",
    "history = multi_lstm_cnn.fit(X_multi, y_multi, batch_size=batch_size, epochs=MAX_EPOCHS,\n",
    "                    validation_steps=10\n",
    "                       )\n",
    "multi_test_performance['LSTM-CNN'] = multi_lstm_cnn.evaluate(test_data_multi, verbose=0, steps=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
